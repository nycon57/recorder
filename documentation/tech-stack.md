Tech Stack and Services

This document enumerates all major technology choices for the project – front-end, back-end, database, hosting, and external services – along with the rationale behind each choice and notes on scalability. Our stack is chosen to accelerate development while ensuring we can scale to meet user demand.

Frontend
	•	Framework: Next.js (latest version with the App Router) with React and TypeScript. We chose Next.js for its hybrid rendering capabilities (static generation for marketing pages, server-side rendering for dynamic app pages) and its first-class support for App Router which simplifies routing and layouts. React with TypeScript provides a robust developer experience, catching errors early and enabling rich IDE support.
	•	UI & Styling: Tailwind CSS (utility-first CSS) for rapid UI development and consistency. Paired with a ShadCN to build accessible components quickly. This combination lets us maintain a consistent design system and easily adjust to branding needs.
	•	State Management: Rely primarily on React’s built-in state and Context. Next.js App Router encourages using React context or server components for data fetching. If needed, we may introduce a library like Zustand or Redux for global state, but initial complexity is kept low by leveraging React and Next features.
	•	Recording Interface: We utilize the MediaDevices API (getUserMedia/getDisplayMedia) and MediaRecorder in the browser to capture screen, camera, and microphone. The front-end includes a custom recording UI that guides the user to grant screen/camera permissions and shows recording controls (start/stop, possibly pause). For combined screen + webcam recording, we capture both streams and composite them (for example, drawing webcam video on a canvas over the screen video to produce a single stream). This is done to ensure the screen and face-cam are synchronized in one video file.
	•	Frontend Build & Tooling: The project uses modern toolchain (ESLint, Prettier, Jest/React Testing Library for testing components). Next.js handles bundling via Webpack/Turbopack, and we can leverage Vercel’s optimization for images and assets.

Rationale: Next.js accelerates development by handling common web app requirements (routing, code-splitting, API routes) and is well-suited for our SaaS which has both a public website and an application. Tailwind and ready-made UI components speed up styling while ensuring a polished, responsive UI out-of-the-box. Using the native MediaRecorder API avoids heavy dependencies and gives us flexibility; it’s supported on modern browsers and lets us stream data for large recordings rather than load entire files in memory.

Backend
	•	Platform: Next.js API Routes / Route Handlers for building our HTTP API. Since our app is primarily serverless and front-end driven, we use Next’s built-in API layer for convenience. This allows co-locating backend endpoint code with the front-end project, simplifying integration.
	•	Language: TypeScript on the server as well (Node.js runtime) for consistency with the front-end and to catch type issues across the stack.
	•	Web Server / Hosting: Vercel is used for hosting the Next.js application (both front-end and API routes). Vercel provides automatic scaling, edge network, and CI/CD integration. It’s optimized for Next.js, which means easy deployments and fast global delivery for our static content.
	•	Asynchronous Workers: For long-running processes (transcription, document generation, etc.), we decouple from the request/response cycle. There are two approaches:
	•	Serverless Tasks: We initiate work in an API route (or Vercel Edge Function) and immediately return, then use callbacks/webhooks to continue processing. For example, when a video is uploaded, an API route can enqueue a transcription job and return quickly. The actual transcription is done by an external service or background process, which calls our webhook when done.
	•	Dedicated Worker Service: As we scale, we may introduce a separate background worker service (e.g. a Node.js or Python process) that pulls jobs from a queue (like Redis or Supabase PG listen/notify) and processes them. This service can run on AWS (Lambda, ECS) or another environment. Initially, we try to leverage external APIs (OpenAI, etc.) that are asynchronous so that our own infrastructure can stay minimal.
	•	API Design: We adopt RESTful endpoints within Next.js for simplicity (detailed in api-spec.md). We also use Edge Middleware (via Clerk and custom logic) for tasks like authentication and rate-limiting where low-latency filtering is needed.

Rationale: Keeping the backend within Next.js (at least initially) reduces complexity – we don’t maintain a separate Express server. Vercel’s serverless model means we don’t worry about managing servers for the API, and it scales automatically per request. By offloading heavy tasks to asynchronous workflows or third-party services, we avoid serverless timeouts and keep the app responsive. This setup is very scalable for bursty workloads – Vercel can spawn more lambdas for API routes as needed – and we’ll design our system to be stateless so this scaling is seamless. If needed, introducing a dedicated worker service for background jobs will further improve throughput without impacting user-facing performance.

Database
	•	Primary Database: PostgreSQL 15 (with the pgvector extension for embeddings). We use a relational database to store structured application data (users, organizations, recordings, transcripts, etc.). The pgvector extension allows us to store and query embedding vectors for our AI search features directly in Postgres.
	•	Hosting: Supabase (managed Postgres) or Amazon RDS for Postgres. Supabase is an attractive choice as it provides a hosted Postgres with pgvector support out-of-the-box, plus convenience features like row level security (RLS) and a built-in API if needed. It also bundles nicely with storage. Alternatively, we could use RDS or Cloud SQL and self-manage the vector extension.
	•	Connection & ORM: We use a query builder/ORM such as Prisma or Knex for type-safe database queries, or SQL directly for full control especially when using vector similarity queries. Prisma has preview support for pgvector, which could simplify integration. We must ensure efficient queries for vector search (HNSW index) and typical relational data.

Rationale: PostgreSQL is a proven, robust database that covers our needs for structured data and now unstructured vector search. By using Postgres for both standard data and embeddings, we reduce operational complexity (one database to maintain) and avoid data synchronization issues between separate systems. This choice is cost-effective as well – early benchmarks show pgvector can handle moderate vector workloads with high performance and low cost, even outperforming some specialized vector DBs at our scale. As we scale, if we encounter performance limits or operational challenges, we can partition data or consider moving to a specialized vector DB (as discussed in vector-strategy.md). In the meantime, a single Postgres simplifies our stack and leverages our team’s familiarity with SQL.

Storage & File Handling
	•	Video/Audio Storage: Original recordings (video files) are stored in object storage. We are considering Supabase Storage (which is S3 under the hood) versus AWS S3 directly. Both provide durable, scalable object storage. Supabase Storage offers a simple integration (we can manage buckets and permissions in the same dashboard as our DB) and even supports resumable uploads and 50GB file sizes now. Alternatively, using S3 via AWS SDK gives us more direct control and is practically the same underlying technology.
	•	CDN: Whichever storage we use, the files are served via a CDN (Supabase uses CDN by default; with S3 we’d front it with CloudFront or use Vercel’s built-in asset proxies). This ensures fast playback/download of videos for end users.
	•	Transcripts & Derived Docs Storage: These are textual and stored in the database (as text columns or possibly in a text search index for backup). We do not expect these to be huge (relative to video), so DB storage is fine. We will also keep vector embeddings in the DB (as part of the pgvector setup).
	•	Backups: Rely on managed service backups (Supabase daily backups for DB; versioning on S3 for storage, etc.) and potentially periodic exports for redundancy.

Rationale: Using a managed storage solution like Supabase or S3 means we don’t worry about capacity – we get virtually infinite storage and high availability. Supabase Storage is essentially S3 with a convenient API and built-in auth integration, which speeds up development (no need to implement our own signing if we use their client libraries). On the other hand, direct S3 might be beneficial if we want fine-grained control or our own AWS account isolation; it also has a rich ecosystem of tools. Both options are scalable; we lean towards starting with Supabase Storage for simplicity and fewer moving parts, and we can switch to raw S3 if needed (since Supabase can export buckets, etc.). The CDN ensures that as we scale to many users or global users, video content is delivered quickly from edge locations.

External Services
	•	User Authentication: Clerk is our auth provider. Clerk manages user sign-up, login, multi-factor, social logins, and its Organization feature gives us ready-made support for multi-tenant (B2B) scenarios (multiple users under organizations with roles). This saves us from building authentication and user management from scratch, and provides a secure, maintained solution.
	•	Payments: Stripe is used for billing. We’ll integrate Stripe for subscription management (plan tiers) and possibly usage-based billing if needed. Stripe’s reliability and ecosystem (Checkout, customer portal, etc.) let us implement billing with minimal custom code. It scales from startup to enterprise seamlessly.
	•	Transcription Service: We use a speech-to-text API for transcribing audio. Options include OpenAI Whisper API, AssemblyAI, or Google Cloud Speech-to-Text. Currently we favor OpenAI Whisper API for its accuracy and reasonable cost, processing the audio to text automatically. If the recording audio is large, we upload it to storage and provide a URL to the transcription service for asynchronous processing (AssemblyAI, for example, can take a file URL and callback). This offloads heavy compute from our servers.
	•	LLM for Document Generation: We leverage OpenAI GPT-5 Nano or GPT-3.5 to convert raw transcripts into structured documentation. After transcription, our backend calls OpenAI’s completion API with a prompt to produce a well-formatted Markdown or outline based on the transcript. We may start with GPT-3.5 (for cost efficiency) and allow opting into GPT-5 Nano for higher quality summarization. This service is external but crucial to our “Docify” feature.
	•	Embedding Vector Generation: We use OpenAI’s text-embedding-3-small model to generate high-dimensional embeddings from transcript chunks or docs. This model returns a 1536-dimension vector that we store in Postgres. It’s fast and reasonably priced, and ensures our vector search (in Postgres or Pinecone) has state-of-the-art semantic representations. We could also consider local embedding models (like sentence-transformers) later for cost savings, but OpenAI’s offering helps us start quickly and with strong performance.
	•	Vector Search (Future): If/when we migrate to Pinecone for vector search, Pinecone itself is an external service. It provides a fully-managed vector database with an easy-to-use API, high performance, and scaling to billions of vectors. We will use it when our Postgres approach needs a boost (see vector-strategy.md for details).
	•	Email/Notifications: For system emails (invites, passwordless links from Clerk, etc.), Clerk handles most by default. If we need custom emails (e.g. “Your document is ready”), we can integrate an email API like SendGrid or use Clerk’s notification hooks. This ensures reliable email delivery at scale.
	•	Analytics & Monitoring: We include services like Sentry for error monitoring and LogRocket or Vercel Analytics for client-side performance/error tracking. These help us maintain quality as we scale. For product analytics (feature usage, funnel tracking), we might use PostHog or a simple integration with e.g. Google Analytics for marketing pages. These external tools ensure we can debug issues and understand usage in production.

Rationale: By leveraging best-of-breed external services, we can implement complex functionality with minimal custom code. Clerk provides a secure auth system with features like org management that would be time-consuming to build ourselves ￼. Stripe handles the nuances of subscription billing and taxes. OpenAI’s models allow us to implement AI features (transcription, summarization, semantic search) that would be infeasible to build from scratch. Using these services allows us to move fast and deliver a high-quality product from day one, and each of them scales far beyond our initial needs. We are mindful of vendor lock-in and cost: our architecture keeps data (transcripts, vectors) in our database so we can switch providers if needed (for example, we could swap out OpenAI embeddings for an open-source model later). Each choice will be continuously evaluated as we grow, but this stack gives us an excellent balance of development speed, capability, and scalability at the start.

Scalability Considerations

Each component of our tech stack has a path to scale:
	•	Next.js/Vercel: Scales automatically for traffic. We should ensure our API routes are stateless and optimize cold start where possible (using edge functions for latency-critical routes). We may introduce caching (Vercel’s Edge Cache or CDN) for public content and even certain API responses to handle high read load.
	•	Postgres (Supabase): We start with a single-node that can handle our initial workload. As usage grows, we can scale vertically (more CPU/RAM, use Supabase’s higher tiers) and horizontally with read replicas. We will optimize queries (indexes, partitioning if needed for very large tables). If write load grows, we consider sharding by tenant (org) or using a connection pooler like PgBouncer. Supabase automatically handles many optimizations and offers features like caching and full-text search which we can use to offload certain queries.
	•	pgvector/Pinecone: Our vector strategy is to use pgvector now, which is sufficient up to millions of vectors on a decent instance. We will monitor query performance (especially as data grows) – pgvector’s HNSW index gives near-constant-time similarity search and can be tuned. When we near the limit (in terms of memory or maintenance complexity) or need multi-region vector queries, we will migrate to Pinecone, which can scale to billion-scale vectors easily. Pinecone will let us distribute by namespace (org) and not worry about index maintenance at large scale. This two-stage approach (pgvector then Pinecone) ensures we use the right tool at the right time, balancing cost and performance.
	•	Storage (videos): Object storage (S3/Supabase) is effectively infinitely scalable. We will implement chunked uploads so even very large files can be handled, and use CDN delivery so load on origin is minimized. We might set up lifecycle rules (e.g. archive or delete old raw videos if not needed) to control costs in the long term. If user base grows globally, we could consider multi-region storage or additional CDN layers, but that is likely handled by our provider.
	•	External services: All chosen services (Clerk, Stripe, OpenAI, etc.) are designed to scale for huge workloads. We will just need to manage rate limits and quotas (for OpenAI, we’ll apply for rate limit increases as needed, or use multiple API keys/ accounts if appropriate). We also keep an eye on cost as usage grows (e.g. OpenAI embedding costs linearly with content volume; we may incorporate caching or move to open-source models internally if that becomes more economical at scale).
	•	Modularity: Our stack is modular – for example, if one part becomes a bottleneck, we can replace it. The app’s architecture (detailed in other docs) ensures we aren’t tightly coupling logic to a single provider. This means we can introduce services (like a dedicated search service, or a separate microservice for heavy AI tasks) without a complete rewrite.

In summary, our tech stack leverages modern frameworks and services to deliver a feature-rich product quickly. Each choice is backed by rationale focusing on developer productivity and the ability to serve our users reliably. As we grow, the same choices provide paths to scale, whether by configuration, paid upgrades, or swapping in more powerful specialized components. This balanced approach ensures we can focus on building unique value (expert recordings to knowledge) rather than reinventing wheels.
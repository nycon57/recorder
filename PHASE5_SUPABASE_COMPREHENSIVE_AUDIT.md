# Phase 5: Connector System - Comprehensive Supabase Audit

**Project:** Recorder - AI Knowledge Management Platform
**Audit Date:** 2025-10-13
**Auditor:** Claude (Supabase Specialist)
**Project ID:** clpatptmumyasbypvmun
**Scope:** Phase 5 Connector System Database Schema, RLS Policies, Storage, and Functions

---

## üîç Executive Summary

This comprehensive audit reviews the Phase 5 connector system implementation, which introduces a plugin-based architecture for importing content from external sources (Google Drive, Notion, Zoom, Teams, file uploads, and URL imports). The audit identifies **CRITICAL GAPS** between the planned Phase 5 schema (documented in `PHASE_5_CONNECTOR_SYSTEM_COMPREHENSIVE.md`) and the currently implemented schema (migration `012_phase1_foundation_enhancements.sql`).

### Audit Findings Overview

| Category | Status | Critical Issues | Warnings | Recommendations |
|----------|--------|-----------------|----------|-----------------|
| Database Schema | ‚ö†Ô∏è **INCOMPLETE** | 5 | 3 | 8 |
| RLS Policies | ‚ö†Ô∏è **GAPS FOUND** | 3 | 2 | 5 |
| Storage Integration | ‚ùå **MISSING** | 2 | 1 | 3 |
| Helper Functions | ‚ùå **NOT IMPLEMENTED** | 2 | 0 | 2 |
| Service Integration | ‚ö†Ô∏è **PARTIAL** | 1 | 2 | 4 |

**Overall Risk Level:** üî¥ **HIGH** - Critical implementation gaps exist that will cause runtime failures

---

## üìä Part 1: Database Schema Analysis

### 1.1 Missing Tables (CRITICAL)

The Phase 5 implementation requires **4 additional tables** that are NOT present in the current database:

#### ‚ùå Missing: `connector_sync_logs`
**Purpose:** Audit logging for all connector sync operations
**Impact:** HIGH - No sync history or debugging capability

```sql
-- REQUIRED TABLE (from Phase 5 spec)
CREATE TABLE connector_sync_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  connector_id UUID REFERENCES connector_configs(id) ON DELETE CASCADE,
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

  -- Sync execution
  sync_type TEXT, -- 'manual', 'scheduled', 'webhook'
  started_at TIMESTAMPTZ DEFAULT now(),
  completed_at TIMESTAMPTZ,
  duration_ms INTEGER,

  -- Results
  status TEXT, -- 'running', 'success', 'partial', 'failed'
  documents_synced INTEGER DEFAULT 0,
  documents_updated INTEGER DEFAULT 0,
  documents_failed INTEGER DEFAULT 0,
  documents_deleted INTEGER DEFAULT 0,

  -- Error tracking
  error_message TEXT,
  error_details JSONB,

  -- Metadata
  metadata JSONB DEFAULT '{}'::jsonb,

  -- Performance metrics
  api_calls_made INTEGER DEFAULT 0,
  bytes_transferred BIGINT DEFAULT 0
);

CREATE INDEX idx_sync_logs_connector ON connector_sync_logs(connector_id, started_at DESC);
CREATE INDEX idx_sync_logs_org ON connector_sync_logs(org_id, started_at DESC);
CREATE INDEX idx_sync_logs_status ON connector_sync_logs(status) WHERE status = 'running';
```

**Service Impact:** `ConnectorManager.logSync()` at line 577 will FAIL - attempting to insert into non-existent table.

#### ‚ùå Missing: `connector_webhook_events`
**Purpose:** Store and process incoming webhook events from external services
**Impact:** HIGH - Real-time integrations (Zoom, Teams) will not work

```sql
-- REQUIRED TABLE (from Phase 5 spec)
CREATE TABLE connector_webhook_events (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  connector_id UUID REFERENCES connector_configs(id) ON DELETE CASCADE,
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

  -- Event info
  event_type TEXT NOT NULL,
  event_source TEXT, -- 'zoom', 'teams', 'google_drive', 'notion'
  event_id TEXT, -- External event ID

  -- Payload
  payload JSONB NOT NULL,
  headers JSONB,

  -- Processing
  processed BOOLEAN DEFAULT false,
  processed_at TIMESTAMPTZ,
  processing_error TEXT,
  retry_count INTEGER DEFAULT 0,

  -- Timestamps
  received_at TIMESTAMPTZ DEFAULT now(),

  -- Deduplication
  UNIQUE (event_source, event_id)
);

CREATE INDEX idx_webhook_events_unprocessed ON connector_webhook_events(processed)
  WHERE NOT processed;
CREATE INDEX idx_webhook_events_received ON connector_webhook_events(received_at DESC);
```

**Service Impact:** Webhook processor will fail, breaking Zoom/Teams real-time sync.

#### ‚ùå Missing: `file_upload_batches`
**Purpose:** Track batch file upload progress and status
**Impact:** HIGH - Batch upload feature completely broken

```sql
-- REQUIRED TABLE (from Phase 5 spec)
CREATE TABLE file_upload_batches (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE SET NULL,

  -- Batch info
  batch_name TEXT,
  total_files INTEGER NOT NULL,
  processed_files INTEGER DEFAULT 0,
  failed_files INTEGER DEFAULT 0,

  -- Status
  status TEXT DEFAULT 'uploading', -- 'uploading', 'processing', 'completed', 'failed'

  -- Progress
  progress_percent FLOAT DEFAULT 0,

  -- Metadata
  metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT now(),
  completed_at TIMESTAMPTZ
);

CREATE INDEX idx_upload_batches_org ON file_upload_batches(org_id, created_at DESC);
CREATE INDEX idx_upload_batches_status ON file_upload_batches(status)
  WHERE status IN ('uploading', 'processing');
```

**Service Impact:**
- `BatchUploader.createBatchRecord()` (line 311) - INSERT will FAIL
- `BatchUploader.updateBatchRecord()` (line 343) - UPDATE will FAIL
- `BatchUploader.getBatchStats()` (line 448) - SELECT will FAIL

#### ‚ö†Ô∏è Incomplete: `connector_configs`
**Current State:** EXISTS in migration 012 but MISSING critical fields
**Impact:** MEDIUM - Service will fail when accessing missing columns

**Missing Fields:**
```sql
-- MISSING from current schema:
credentials_updated_at TIMESTAMPTZ,  -- Used by ConnectorManager line 166
next_sync_at TIMESTAMPTZ,            -- Used for scheduled syncs
sync_frequency TEXT,                  -- 'manual', 'hourly', 'daily', 'weekly'
last_sync_status TEXT,               -- Used to track partial/failed syncs
webhook_url TEXT,                    -- Required for webhook setup
webhook_secret TEXT,                 -- Webhook authentication
webhook_active BOOLEAN,              -- Webhook enable/disable
error_count INTEGER DEFAULT 0,       -- Error tracking
last_error TEXT,                     -- Detailed error messages
last_error_at TIMESTAMPTZ,          -- Error timestamp
description TEXT,                    -- Connector description
filters JSONB DEFAULT '{}'::jsonb,  -- File type/folder filters
metadata JSONB DEFAULT '{}'::jsonb  -- Additional metadata
```

**Field Comparison:**

| Field | Current Schema | Phase 5 Spec | Impact |
|-------|---------------|--------------|--------|
| `credentials_updated_at` | ‚ùå Missing | ‚úÖ Required | HIGH - UpdateConnector fails (line 166) |
| `sync_frequency` | ‚ùå Missing | ‚úÖ Required | MEDIUM - Scheduled syncs won't work |
| `next_sync_at` | ‚ùå Missing | ‚úÖ Required | MEDIUM - Scheduler can't queue syncs |
| `last_sync_status` | ‚ùå Missing | ‚úÖ Required | LOW - Can derive from logs |
| `webhook_*` fields | ‚ùå Missing | ‚úÖ Required | HIGH - Webhooks completely broken |
| `error_count` | ‚ùå Missing | ‚úÖ Required | LOW - Nice to have for monitoring |
| `filters` | ‚ùå Missing | ‚úÖ Required | MEDIUM - Can't filter sync scope |

#### ‚ö†Ô∏è Incomplete: `imported_documents`
**Current State:** EXISTS but MISSING critical fields
**Impact:** MEDIUM - Advanced features won't work

**Missing Fields:**
```sql
-- MISSING from current schema:
content_hash TEXT,                   -- For duplicate detection
parent_external_id TEXT,             -- For hierarchical sources (folders)
processing_status TEXT,              -- Replaces sync_status (Phase 5 naming)
processing_error TEXT,               -- Replaces sync_error
chunks_generated BOOLEAN,            -- Track chunking progress
embeddings_generated BOOLEAN,        -- Track embedding progress
source_metadata JSONB,               -- Source system metadata
first_synced_at TIMESTAMPTZ,        -- Initial sync timestamp
sync_count INTEGER,                  -- Number of times synced
is_deleted BOOLEAN,                  -- Soft delete flag
external_url TEXT                    -- Link to source
```

**Critical Gap:** Current schema has `sync_status` but Phase 5 uses `processing_status` - This mismatch will cause query failures!

### 1.2 Schema Validation Checklist

| Requirement | Status | Notes |
|-------------|--------|-------|
| All required tables exist | ‚ùå FAIL | 3 missing tables |
| All required columns exist | ‚ùå FAIL | 15+ missing columns |
| Foreign key constraints correct | ‚úÖ PASS | Existing constraints valid |
| Unique constraints present | ‚ö†Ô∏è PARTIAL | Missing on webhook events |
| Check constraints defined | ‚ö†Ô∏è PARTIAL | Missing on new tables |
| Proper data types used | ‚úÖ PASS | Existing types correct |
| Default values appropriate | ‚úÖ PASS | Existing defaults good |
| Naming conventions consistent | ‚ö†Ô∏è PARTIAL | `sync_status` vs `processing_status` mismatch |

---

## üîê Part 2: Row Level Security (RLS) Analysis

### 2.1 Current RLS Policy Coverage

#### ‚úÖ `connector_configs` - PROPERLY SECURED
**Policies Implemented (from migration 016):**

```sql
-- READ access
CREATE POLICY "Users can view their org's connectors"
  ON connector_configs FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

-- WRITE access (all operations)
CREATE POLICY "Users can create connectors for their org"
  ON connector_configs FOR INSERT
  WITH CHECK (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Users can update their org's connectors"
  ON connector_configs FOR UPDATE
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Users can delete their org's connectors"
  ON connector_configs FOR DELETE
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));
```

**Issues:**
1. ‚ö†Ô∏è **No Role-Based Access Control** - Phase 5 spec requires admin/owner restriction for management
2. ‚ö†Ô∏è **No Service Role Bypass** - Background workers need explicit service_role policy

**Recommended Fix:**
```sql
-- Replace write policies with role-based access
DROP POLICY IF EXISTS "Users can create connectors for their org" ON connector_configs;
DROP POLICY IF EXISTS "Users can update their org's connectors" ON connector_configs;
DROP POLICY IF EXISTS "Users can delete their org's connectors" ON connector_configs;

-- Only admins/owners can manage connectors
CREATE POLICY "Admins can manage their org's connectors"
  ON connector_configs FOR ALL
  USING (
    org_id IN (
      SELECT org_id FROM users
      WHERE id = auth.uid()
        AND role IN ('owner', 'admin')
    )
  )
  WITH CHECK (
    org_id IN (
      SELECT org_id FROM users
      WHERE id = auth.uid()
        AND role IN ('owner', 'admin')
    )
  );

-- Service role full access for background jobs
CREATE POLICY "Service role full access to connectors"
  ON connector_configs FOR ALL TO service_role
  USING (true) WITH CHECK (true);
```

#### ‚úÖ `imported_documents` - PROPERLY SECURED (Read-only)
**Current Policy (from migration 016):**

```sql
CREATE POLICY "Users can view their org's imported documents"
  ON imported_documents FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));
```

**Issues:**
1. ‚ùå **No Service Role Policy** - Background workers can't insert/update documents
2. ‚ö†Ô∏è **Users can't manage their own uploads** - No DELETE policy for user-uploaded files

**Critical Gap:** Service attempting to insert via `supabaseAdmin` will work, but any client-side operations will fail.

**Recommended Fix:**
```sql
-- Service role full access (CRITICAL for background jobs)
CREATE POLICY "Service role full access to imported docs"
  ON imported_documents FOR ALL TO service_role
  USING (true) WITH CHECK (true);

-- Users can delete their own uploaded documents (optional)
CREATE POLICY "Users can delete their org's imported documents"
  ON imported_documents FOR DELETE
  USING (
    org_id IN (SELECT org_id FROM users WHERE id = auth.uid())
    AND (
      -- Owner/admin can delete any
      (SELECT role FROM users WHERE id = auth.uid()) IN ('owner', 'admin')
      OR
      -- Others can only delete if they uploaded it
      metadata->>'uploadedBy' = auth.uid()::text
    )
  );
```

#### ‚ùå `connector_sync_logs` - NOT IMPLEMENTED
**Impact:** HIGH - Table doesn't exist, so no policies

**Required Policies:**
```sql
ALTER TABLE connector_sync_logs ENABLE ROW LEVEL SECURITY;

-- Users can view sync logs for their org's connectors
CREATE POLICY "Users can view their org's sync logs"
  ON connector_sync_logs FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

-- Service role can insert/update logs
CREATE POLICY "Service role manages sync logs"
  ON connector_sync_logs FOR ALL TO service_role
  USING (true) WITH CHECK (true);
```

#### ‚ùå `connector_webhook_events` - NOT IMPLEMENTED
**Impact:** HIGH - Real-time webhooks will fail

**Required Policies:**
```sql
ALTER TABLE connector_webhook_events ENABLE ROW LEVEL SECURITY;

-- Users can view webhook events for their org
CREATE POLICY "Users can view their org's webhook events"
  ON connector_webhook_events FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

-- Service role processes webhooks
CREATE POLICY "Service role manages webhook events"
  ON connector_webhook_events FOR ALL TO service_role
  USING (true) WITH CHECK (true);
```

#### ‚ùå `file_upload_batches` - NOT IMPLEMENTED
**Impact:** HIGH - Batch uploads completely broken

**Required Policies:**
```sql
ALTER TABLE file_upload_batches ENABLE ROW LEVEL SECURITY;

-- Users can view their org's upload batches
CREATE POLICY "Users can view their org's upload batches"
  ON file_upload_batches FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

-- Users can create upload batches
CREATE POLICY "Users can create upload batches"
  ON file_upload_batches FOR INSERT
  WITH CHECK (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

-- Service role manages batch processing
CREATE POLICY "Service role manages upload batches"
  ON file_upload_batches FOR ALL TO service_role
  USING (true) WITH CHECK (true);
```

### 2.2 RLS Security Audit Summary

| Table | RLS Enabled | Policies Count | Service Role | Status |
|-------|-------------|----------------|--------------|--------|
| `connector_configs` | ‚úÖ Yes | 4 | ‚ùå Missing | ‚ö†Ô∏è GAPS |
| `imported_documents` | ‚úÖ Yes | 1 | ‚ùå Missing | ‚ö†Ô∏è GAPS |
| `connector_sync_logs` | ‚ùå N/A | 0 | ‚ùå N/A | ‚ùå MISSING |
| `connector_webhook_events` | ‚ùå N/A | 0 | ‚ùå N/A | ‚ùå MISSING |
| `file_upload_batches` | ‚ùå N/A | 0 | ‚ùå N/A | ‚ùå MISSING |

**Critical Finding:** All background job operations will FAIL without service_role policies!

---

## üíæ Part 3: Storage Integration Analysis

### 3.1 Storage Bucket Configuration

#### Current Implementation
The services reference storage buckets that may not be properly configured:

**BatchUploader (line 240):**
```typescript
await supabaseAdmin.storage
  .from('recordings')  // ‚ö†Ô∏è Using 'recordings' bucket
  .upload(storagePath, file.buffer, {
    contentType: file.mimeType,
    upsert: false,
    cacheControl: '3600',
  });
```

**Issues:**
1. ‚ùå **No dedicated connector storage bucket** - Phase 5 should have separate `connector-imports` bucket
2. ‚ö†Ô∏è **Mixing recording and imported files** - Hard to manage quotas and cleanup
3. ‚ùå **No storage policies defined** for connector uploads

#### ‚ùå Missing: Storage Bucket for Connectors

**Required Storage Setup:**
```sql
-- Create dedicated connector imports bucket
INSERT INTO storage.buckets (id, name, public)
VALUES ('connector-imports', 'connector-imports', false);

-- RLS policies for connector-imports bucket
CREATE POLICY "Users can view their org's connector files"
  ON storage.objects FOR SELECT
  USING (
    bucket_id = 'connector-imports'
    AND (storage.foldername(name))[1] IN (
      SELECT org_id::text FROM users WHERE id = auth.uid()
    )
  );

CREATE POLICY "Service role manages connector files"
  ON storage.objects FOR ALL TO service_role
  USING (bucket_id = 'connector-imports')
  WITH CHECK (bucket_id = 'connector-imports');
```

### 3.2 File Path Conventions

**Current Pattern (BatchUploader):**
```
org_${orgId}/batch_${batchId}/${fileId}-${filename}
```

**Issues:**
1. ‚ö†Ô∏è **No connector-specific namespacing** - Can't identify which connector imported a file
2. ‚ö†Ô∏è **No date-based partitioning** - Difficult to manage old imports
3. ‚ùå **No file type segregation** - Videos, docs, images all mixed together

**Recommended Pattern:**
```
{orgId}/connectors/{connectorType}/{connectorId}/{YYYY-MM-DD}/{fileType}/{fileId}-{filename}

Example:
abc123/connectors/google_drive/conn-456/2025-10-13/documents/file-789-report.pdf
abc123/connectors/zoom/conn-123/2025-10-13/videos/file-456-meeting-recording.mp4
```

**Benefits:**
- Easy to identify source connector
- Date-based partitioning for cleanup
- File type organization for processing pipelines
- Better analytics on connector usage

### 3.3 Storage Cleanup Strategy

**Missing Implementation:**

```sql
-- Cleanup function for old connector imports
CREATE OR REPLACE FUNCTION cleanup_old_connector_imports(
  days_old INTEGER DEFAULT 90
)
RETURNS TABLE (
  deleted_files INTEGER,
  storage_freed_bytes BIGINT
) AS $$
DECLARE
  v_deleted_files INTEGER := 0;
  v_storage_freed BIGINT := 0;
BEGIN
  -- Mark old imported documents as deleted
  WITH deleted AS (
    UPDATE imported_documents
    SET is_deleted = true
    WHERE last_synced_at < now() - (days_old || ' days')::interval
      AND NOT is_deleted
      AND connector_id IS NOT NULL
    RETURNING id, file_size_bytes
  )
  SELECT COUNT(*), COALESCE(SUM(file_size_bytes), 0)
  INTO v_deleted_files, v_storage_freed
  FROM deleted;

  -- Delete storage files (would need separate storage cleanup job)

  RETURN QUERY SELECT v_deleted_files, v_storage_freed;
END;
$$ LANGUAGE plpgsql;

-- Schedule cleanup (requires pg_cron extension)
-- SELECT cron.schedule('cleanup-old-imports', '0 2 * * *',
--   'SELECT cleanup_old_connector_imports(90)');
```

### 3.4 Storage Audit Summary

| Aspect | Status | Impact | Priority |
|--------|--------|--------|----------|
| Dedicated connector bucket | ‚ùå Missing | HIGH | P0 |
| Storage RLS policies | ‚ùå Missing | HIGH | P0 |
| File path conventions | ‚ö†Ô∏è Suboptimal | MEDIUM | P1 |
| Cleanup strategy | ‚ùå Missing | MEDIUM | P1 |
| Quota management | ‚ùå Missing | LOW | P2 |
| CDN/caching setup | ‚ùå Missing | LOW | P3 |

---

## ‚öôÔ∏è Part 4: Database Functions Analysis

### 4.1 Missing Helper Functions

The `BatchUploader` service references functions that DO NOT EXIST:

#### ‚ùå Missing: `increment_batch_processed`
**Referenced at:** `lib/services/batch-uploader.ts:1451`

```typescript
await supabase.rpc('increment_batch_processed', {
  batch_id_param: batchId,
});
```

**Impact:** HIGH - Batch progress tracking will FAIL

**Required Implementation:**
```sql
CREATE OR REPLACE FUNCTION increment_batch_processed(
  batch_id_param UUID
)
RETURNS void AS $$
BEGIN
  UPDATE file_upload_batches
  SET
    processed_files = processed_files + 1,
    progress_percent = CASE
      WHEN total_files > 0
      THEN ((processed_files + 1)::float / total_files::float) * 100
      ELSE 0
    END,
    updated_at = now()
  WHERE id = batch_id_param;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Grant execute to service role
GRANT EXECUTE ON FUNCTION increment_batch_processed TO service_role;

COMMENT ON FUNCTION increment_batch_processed IS
  'Atomically increment processed file count for upload batch';
```

#### ‚ùå Missing: `increment_batch_failed`
**Referenced at:** `lib/services/batch-uploader.ts:1458`

```typescript
await supabase.rpc('increment_batch_failed', {
  batch_id_param: batchId,
});
```

**Impact:** HIGH - Failure tracking will FAIL

**Required Implementation:**
```sql
CREATE OR REPLACE FUNCTION increment_batch_failed(
  batch_id_param UUID
)
RETURNS void AS $$
BEGIN
  UPDATE file_upload_batches
  SET
    failed_files = failed_files + 1,
    processed_files = processed_files + 1,
    progress_percent = CASE
      WHEN total_files > 0
      THEN ((processed_files + 1)::float / total_files::float) * 100
      ELSE 0
    END,
    status = CASE
      WHEN (failed_files + 1) >= total_files THEN 'failed'
      ELSE status
    END,
    updated_at = now()
  WHERE id = batch_id_param;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Grant execute to service role
GRANT EXECUTE ON FUNCTION increment_batch_failed TO service_role;

COMMENT ON FUNCTION increment_batch_failed IS
  'Atomically increment failed file count for upload batch';
```

### 4.2 Missing Search Functions

Phase 5 documents may need to be searchable alongside recordings. Current `hierarchical_search` function only searches `transcript_chunks`.

**Required Enhancement:**
```sql
-- Unified search across recordings AND imported documents
CREATE OR REPLACE FUNCTION unified_content_search(
  query_embedding vector(1536),
  match_org_id UUID,
  match_count INTEGER DEFAULT 20,
  match_threshold FLOAT DEFAULT 0.7,
  include_imported BOOLEAN DEFAULT true
)
RETURNS TABLE (
  id UUID,
  source_type TEXT, -- 'recording' or 'imported_document'
  source_id UUID,
  title TEXT,
  content TEXT,
  similarity FLOAT,
  metadata JSONB,
  created_at TIMESTAMPTZ
) AS $$
BEGIN
  RETURN QUERY
  -- Search transcript chunks
  SELECT
    tc.id,
    'recording'::TEXT as source_type,
    tc.recording_id as source_id,
    r.title,
    tc.chunk_text as content,
    1 - (tc.embedding <=> query_embedding) as similarity,
    tc.metadata,
    tc.created_at
  FROM transcript_chunks tc
  INNER JOIN recordings r ON tc.recording_id = r.id
  WHERE tc.org_id = match_org_id
    AND 1 - (tc.embedding <=> query_embedding) >= match_threshold

  UNION ALL

  -- Search imported documents (if enabled)
  SELECT
    id.id,
    'imported_document'::TEXT as source_type,
    id.connector_id as source_id,
    id.title,
    id.content,
    ts_rank_cd(
      to_tsvector('english', id.content),
      plainto_tsquery('english', query_text)
    ) as similarity,
    id.metadata,
    id.created_at
  FROM imported_documents id
  WHERE include_imported
    AND id.org_id = match_org_id
    AND id.content IS NOT NULL
    AND id.processing_status = 'completed'

  ORDER BY similarity DESC
  LIMIT match_count;
END;
$$ LANGUAGE plpgsql STABLE;
```

### 4.3 Functions Audit Summary

| Function | Status | Referenced By | Impact |
|----------|--------|---------------|--------|
| `increment_batch_processed` | ‚ùå Missing | BatchUploader:1451 | HIGH |
| `increment_batch_failed` | ‚ùå Missing | BatchUploader:1458 | HIGH |
| `unified_content_search` | ‚ùå Missing | Future search API | MEDIUM |
| `cleanup_old_connector_imports` | ‚ùå Missing | Maintenance | LOW |

---

## üîó Part 5: Service Integration Analysis

### 5.1 ConnectorManager Service Issues

**File:** `lib/services/connector-manager.ts`

#### Issue 1: Missing `credentials_updated_at` Column
**Location:** Line 166
```typescript
updateData.credentials_updated_at = new Date().toISOString();
```
**Impact:** HIGH - UPDATE query will FAIL
**Fix:** Add column to migration or remove this line

#### Issue 2: Missing Sync Log Table
**Location:** Line 577
```typescript
await supabaseAdmin.from('connector_sync_logs').insert({
  connector_id: log.connectorId,
  // ... other fields
});
```
**Impact:** HIGH - INSERT will FAIL
**Fix:** Create `connector_sync_logs` table

#### Issue 3: No Error Handling for Missing Service Role Policy
**Location:** Lines 95-108, 577-596
```typescript
await supabaseAdmin.from('connector_configs').insert({
  // ... using admin client but no service_role policy
});
```
**Impact:** MEDIUM - Will work with supabaseAdmin but inconsistent
**Fix:** Add explicit service_role policies

### 5.2 BatchUploader Service Issues

**File:** `lib/services/batch-uploader.ts`

#### Issue 1: References Non-Existent Table
**Location:** Line 311
```typescript
await supabaseAdmin.from('file_upload_batches').insert({
  id: this.batchId,
  org_id: this.orgId,
  // ...
});
```
**Impact:** CRITICAL - INSERT will FAIL
**Fix:** Create `file_upload_batches` table

#### Issue 2: Missing RPC Functions
**Location:** Lines 1451, 1458
```typescript
await supabase.rpc('increment_batch_processed', { batch_id_param: batchId });
await supabase.rpc('increment_batch_failed', { batch_id_param: batchId });
```
**Impact:** CRITICAL - RPC calls will FAIL
**Fix:** Create both functions

#### Issue 3: Schema Mismatch
**Location:** Line 396
```typescript
sync_status: this.autoProcess ? 'pending' : 'completed',
```
**Expected:** `processing_status` (Phase 5 naming)
**Impact:** MEDIUM - Field doesn't exist in Phase 5 schema
**Fix:** Use `processing_status` instead

#### Issue 4: Missing `content_hash` Field
**Location:** Line 382-384
```typescript
const contentHash = createHash('sha256')
  .update(file.buffer)
  .digest('hex');
// ... but then tries to insert into content_hash column that doesn't exist
```
**Impact:** MEDIUM - INSERT will include undefined column
**Fix:** Add `content_hash` to `imported_documents` schema

### 5.3 Integration Dependency Map

```
ConnectorManager.createConnector()
  ‚îú‚îÄ> connector_configs (‚úÖ exists, ‚ö†Ô∏è missing columns)
  ‚îî‚îÄ> Connector.authenticate() (‚úÖ works)

ConnectorManager.syncConnector()
  ‚îú‚îÄ> connector_configs (‚úÖ read works)
  ‚îú‚îÄ> Connector.sync() (‚úÖ works)
  ‚îî‚îÄ> connector_sync_logs (‚ùå FAILS - table missing)

BatchUploader.processRequest()
  ‚îú‚îÄ> file_upload_batches (‚ùå FAILS - table missing)
  ‚îú‚îÄ> imported_documents (‚ö†Ô∏è PARTIAL - missing columns)
  ‚îú‚îÄ> increment_batch_processed (‚ùå FAILS - function missing)
  ‚îî‚îÄ> increment_batch_failed (‚ùå FAILS - function missing)

BatchUploader.createDocumentRecord()
  ‚îú‚îÄ> imported_documents.content_hash (‚ùå FAILS - column missing)
  ‚îú‚îÄ> imported_documents.sync_status (‚ö†Ô∏è should be processing_status)
  ‚îî‚îÄ> jobs table (‚úÖ works)
```

---

## üìã Part 6: Migration Recommendations

### 6.1 Critical Priority Migration (P0)

**File:** `supabase/migrations/025_phase5_connector_system_enhancements.sql`

```sql
-- =====================================================
-- Phase 5: Connector System Enhancements
-- Priority: P0 - CRITICAL (breaks existing services)
-- =====================================================

BEGIN;

-- =====================================================
-- 1. UPDATE EXISTING TABLES
-- =====================================================

-- Add missing columns to connector_configs
ALTER TABLE connector_configs
  ADD COLUMN IF NOT EXISTS credentials_updated_at TIMESTAMPTZ,
  ADD COLUMN IF NOT EXISTS next_sync_at TIMESTAMPTZ,
  ADD COLUMN IF NOT EXISTS sync_frequency TEXT DEFAULT 'manual',
  ADD COLUMN IF NOT EXISTS last_sync_status TEXT,
  ADD COLUMN IF NOT EXISTS webhook_url TEXT,
  ADD COLUMN IF NOT EXISTS webhook_secret TEXT,
  ADD COLUMN IF NOT EXISTS webhook_active BOOLEAN DEFAULT false,
  ADD COLUMN IF NOT EXISTS error_count INTEGER DEFAULT 0,
  ADD COLUMN IF NOT EXISTS last_error TEXT,
  ADD COLUMN IF NOT EXISTS last_error_at TIMESTAMPTZ,
  ADD COLUMN IF NOT EXISTS description TEXT,
  ADD COLUMN IF NOT EXISTS filters JSONB DEFAULT '{}'::jsonb,
  ADD COLUMN IF NOT EXISTS metadata JSONB DEFAULT '{}'::jsonb;

-- Add check constraint for sync_frequency
ALTER TABLE connector_configs
  ADD CONSTRAINT check_sync_frequency
  CHECK (sync_frequency IN ('manual', 'hourly', 'daily', 'weekly'));

-- Add index for scheduled syncs
CREATE INDEX IF NOT EXISTS idx_connector_configs_next_sync
  ON connector_configs(next_sync_at)
  WHERE is_active = true AND sync_frequency != 'manual';

-- Add missing columns to imported_documents
ALTER TABLE imported_documents
  ADD COLUMN IF NOT EXISTS content_hash TEXT,
  ADD COLUMN IF NOT EXISTS parent_external_id TEXT,
  ADD COLUMN IF NOT EXISTS processing_status TEXT DEFAULT 'pending',
  ADD COLUMN IF NOT EXISTS processing_error TEXT,
  ADD COLUMN IF NOT EXISTS chunks_generated BOOLEAN DEFAULT false,
  ADD COLUMN IF NOT EXISTS embeddings_generated BOOLEAN DEFAULT false,
  ADD COLUMN IF NOT EXISTS source_metadata JSONB DEFAULT '{}'::jsonb,
  ADD COLUMN IF NOT EXISTS first_synced_at TIMESTAMPTZ DEFAULT now(),
  ADD COLUMN IF NOT EXISTS sync_count INTEGER DEFAULT 1,
  ADD COLUMN IF NOT EXISTS is_deleted BOOLEAN DEFAULT false,
  ADD COLUMN IF NOT EXISTS external_url TEXT;

-- Migrate sync_status to processing_status if needed
UPDATE imported_documents
SET processing_status = sync_status
WHERE processing_status IS NULL AND sync_status IS NOT NULL;

-- Add check constraint
ALTER TABLE imported_documents
  ADD CONSTRAINT check_processing_status
  CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed'));

-- Add indexes
CREATE INDEX IF NOT EXISTS idx_imported_docs_hash
  ON imported_documents(content_hash)
  WHERE content_hash IS NOT NULL;

CREATE INDEX IF NOT EXISTS idx_imported_docs_chunks
  ON imported_documents(chunks_generated)
  WHERE NOT chunks_generated;

CREATE INDEX IF NOT EXISTS idx_imported_docs_deleted
  ON imported_documents(is_deleted)
  WHERE is_deleted = true;

-- =====================================================
-- 2. CREATE NEW TABLES
-- =====================================================

-- Connector Sync Logs
CREATE TABLE IF NOT EXISTS connector_sync_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  connector_id UUID REFERENCES connector_configs(id) ON DELETE CASCADE,
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

  sync_type TEXT,
  started_at TIMESTAMPTZ DEFAULT now(),
  completed_at TIMESTAMPTZ,
  duration_ms INTEGER,

  status TEXT,
  documents_synced INTEGER DEFAULT 0,
  documents_updated INTEGER DEFAULT 0,
  documents_failed INTEGER DEFAULT 0,
  documents_deleted INTEGER DEFAULT 0,

  error_message TEXT,
  error_details JSONB,
  metadata JSONB DEFAULT '{}'::jsonb,

  api_calls_made INTEGER DEFAULT 0,
  bytes_transferred BIGINT DEFAULT 0,

  CONSTRAINT check_sync_type CHECK (sync_type IN ('manual', 'scheduled', 'webhook')),
  CONSTRAINT check_sync_status CHECK (status IN ('running', 'success', 'partial', 'failed'))
);

CREATE INDEX idx_sync_logs_connector ON connector_sync_logs(connector_id, started_at DESC);
CREATE INDEX idx_sync_logs_org ON connector_sync_logs(org_id, started_at DESC);
CREATE INDEX idx_sync_logs_status ON connector_sync_logs(status) WHERE status = 'running';

COMMENT ON TABLE connector_sync_logs IS 'Audit log of all connector sync operations';

-- Connector Webhook Events
CREATE TABLE IF NOT EXISTS connector_webhook_events (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  connector_id UUID REFERENCES connector_configs(id) ON DELETE CASCADE,
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,

  event_type TEXT NOT NULL,
  event_source TEXT,
  event_id TEXT,

  payload JSONB NOT NULL,
  headers JSONB,

  processed BOOLEAN DEFAULT false,
  processed_at TIMESTAMPTZ,
  processing_error TEXT,
  retry_count INTEGER DEFAULT 0,

  received_at TIMESTAMPTZ DEFAULT now(),

  UNIQUE (event_source, event_id)
);

CREATE INDEX idx_webhook_events_unprocessed ON connector_webhook_events(processed)
  WHERE NOT processed;
CREATE INDEX idx_webhook_events_received ON connector_webhook_events(received_at DESC);
CREATE INDEX idx_webhook_events_connector ON connector_webhook_events(connector_id);

COMMENT ON TABLE connector_webhook_events IS 'Incoming webhook events from external services';

-- File Upload Batches
CREATE TABLE IF NOT EXISTS file_upload_batches (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
  user_id UUID REFERENCES users(id) ON DELETE SET NULL,

  batch_name TEXT,
  total_files INTEGER NOT NULL,
  processed_files INTEGER DEFAULT 0,
  failed_files INTEGER DEFAULT 0,

  status TEXT DEFAULT 'uploading',
  progress_percent FLOAT DEFAULT 0,

  metadata JSONB DEFAULT '{}'::jsonb,
  created_at TIMESTAMPTZ DEFAULT now(),
  completed_at TIMESTAMPTZ,

  CONSTRAINT check_batch_status CHECK (status IN ('uploading', 'processing', 'completed', 'failed'))
);

CREATE INDEX idx_upload_batches_org ON file_upload_batches(org_id, created_at DESC);
CREATE INDEX idx_upload_batches_status ON file_upload_batches(status)
  WHERE status IN ('uploading', 'processing');

COMMENT ON TABLE file_upload_batches IS 'Batch file upload tracking and progress';

-- =====================================================
-- 3. CREATE HELPER FUNCTIONS
-- =====================================================

-- Increment batch processed count
CREATE OR REPLACE FUNCTION increment_batch_processed(
  batch_id_param UUID
)
RETURNS void AS $$
BEGIN
  UPDATE file_upload_batches
  SET
    processed_files = processed_files + 1,
    progress_percent = CASE
      WHEN total_files > 0
      THEN ((processed_files + 1)::float / total_files::float) * 100
      ELSE 0
    END
  WHERE id = batch_id_param;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

GRANT EXECUTE ON FUNCTION increment_batch_processed TO service_role;
COMMENT ON FUNCTION increment_batch_processed IS 'Atomically increment processed file count';

-- Increment batch failed count
CREATE OR REPLACE FUNCTION increment_batch_failed(
  batch_id_param UUID
)
RETURNS void AS $$
BEGIN
  UPDATE file_upload_batches
  SET
    failed_files = failed_files + 1,
    processed_files = processed_files + 1,
    progress_percent = CASE
      WHEN total_files > 0
      THEN ((processed_files + 1)::float / total_files::float) * 100
      ELSE 0
    END,
    status = CASE
      WHEN (failed_files + 1) >= total_files THEN 'failed'
      ELSE status
    END
  WHERE id = batch_id_param;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

GRANT EXECUTE ON FUNCTION increment_batch_failed TO service_role;
COMMENT ON FUNCTION increment_batch_failed IS 'Atomically increment failed file count';

-- =====================================================
-- 4. UPDATE RLS POLICIES
-- =====================================================

-- Fix connector_configs policies (add role restriction)
DROP POLICY IF EXISTS "Users can create connectors for their org" ON connector_configs;
DROP POLICY IF EXISTS "Users can update their org's connectors" ON connector_configs;
DROP POLICY IF EXISTS "Users can delete their org's connectors" ON connector_configs;

CREATE POLICY "Admins can manage their org's connectors"
  ON connector_configs FOR ALL
  USING (
    org_id IN (
      SELECT org_id FROM users
      WHERE id = auth.uid()
        AND role IN ('owner', 'admin')
    )
  )
  WITH CHECK (
    org_id IN (
      SELECT org_id FROM users
      WHERE id = auth.uid()
        AND role IN ('owner', 'admin')
    )
  );

CREATE POLICY "Service role full access to connectors"
  ON connector_configs FOR ALL TO service_role
  USING (true) WITH CHECK (true);

-- Fix imported_documents policies
CREATE POLICY "Service role full access to imported docs"
  ON imported_documents FOR ALL TO service_role
  USING (true) WITH CHECK (true);

-- Add policies for new tables
ALTER TABLE connector_sync_logs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their org's sync logs"
  ON connector_sync_logs FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Service role manages sync logs"
  ON connector_sync_logs FOR ALL TO service_role
  USING (true) WITH CHECK (true);

ALTER TABLE connector_webhook_events ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their org's webhook events"
  ON connector_webhook_events FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Service role manages webhook events"
  ON connector_webhook_events FOR ALL TO service_role
  USING (true) WITH CHECK (true);

ALTER TABLE file_upload_batches ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their org's upload batches"
  ON file_upload_batches FOR SELECT
  USING (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Users can create upload batches"
  ON file_upload_batches FOR INSERT
  WITH CHECK (org_id IN (SELECT org_id FROM users WHERE id = auth.uid()));

CREATE POLICY "Service role manages upload batches"
  ON file_upload_batches FOR ALL TO service_role
  USING (true) WITH CHECK (true);

-- =====================================================
-- 5. GRANTS
-- =====================================================

GRANT ALL ON connector_sync_logs TO service_role;
GRANT ALL ON connector_webhook_events TO service_role;
GRANT ALL ON file_upload_batches TO service_role;

-- =====================================================
-- VALIDATION
-- =====================================================

DO $$
BEGIN
  -- Verify all tables exist
  IF NOT EXISTS (SELECT 1 FROM pg_tables WHERE tablename = 'connector_sync_logs') THEN
    RAISE EXCEPTION 'Table connector_sync_logs was not created';
  END IF;

  IF NOT EXISTS (SELECT 1 FROM pg_tables WHERE tablename = 'connector_webhook_events') THEN
    RAISE EXCEPTION 'Table connector_webhook_events was not created';
  END IF;

  IF NOT EXISTS (SELECT 1 FROM pg_tables WHERE tablename = 'file_upload_batches') THEN
    RAISE EXCEPTION 'Table file_upload_batches was not created';
  END IF;

  -- Verify functions exist
  IF NOT EXISTS (SELECT 1 FROM pg_proc WHERE proname = 'increment_batch_processed') THEN
    RAISE EXCEPTION 'Function increment_batch_processed was not created';
  END IF;

  IF NOT EXISTS (SELECT 1 FROM pg_proc WHERE proname = 'increment_batch_failed') THEN
    RAISE EXCEPTION 'Function increment_batch_failed was not created';
  END IF;

  RAISE NOTICE 'Phase 5 migration completed successfully';
  RAISE NOTICE '  - Updated 2 existing tables with missing columns';
  RAISE NOTICE '  - Created 3 new tables';
  RAISE NOTICE '  - Created 2 helper functions';
  RAISE NOTICE '  - Added/updated 10 RLS policies';
END $$;

COMMIT;
```

### 6.2 Storage Configuration Migration (P0)

**File:** `supabase/migrations/025_phase5_storage_configuration.sql`

```sql
-- =====================================================
-- Phase 5: Storage Configuration
-- Priority: P0 - Required for connector file uploads
-- =====================================================

BEGIN;

-- Create connector-imports bucket
INSERT INTO storage.buckets (id, name, public, file_size_limit, allowed_mime_types)
VALUES (
  'connector-imports',
  'connector-imports',
  false,
  5368709120, -- 5GB limit
  ARRAY[
    'video/mp4', 'video/webm', 'video/quicktime',
    'audio/mpeg', 'audio/wav', 'audio/ogg',
    'image/png', 'image/jpeg', 'image/gif', 'image/webp',
    'application/pdf',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    'application/vnd.openxmlformats-officedocument.presentationml.presentation',
    'text/plain', 'text/markdown', 'text/csv',
    'application/json'
  ]
)
ON CONFLICT (id) DO NOTHING;

-- RLS policies for connector-imports bucket
CREATE POLICY "Users can view their org's connector files"
  ON storage.objects FOR SELECT
  USING (
    bucket_id = 'connector-imports'
    AND (storage.foldername(name))[1] IN (
      SELECT org_id::text FROM users WHERE id = auth.uid()
    )
  );

CREATE POLICY "Service role manages connector files"
  ON storage.objects FOR ALL TO service_role
  USING (bucket_id = 'connector-imports')
  WITH CHECK (bucket_id = 'connector-imports');

RAISE NOTICE 'Connector storage bucket created with RLS policies';

COMMIT;
```

### 6.3 Rollback Migration

**File:** `supabase/migrations/025_phase5_connector_system_enhancements_down.sql`

```sql
-- =====================================================
-- Phase 5: Rollback Migration
-- =====================================================

BEGIN;

-- Drop new tables
DROP TABLE IF EXISTS file_upload_batches CASCADE;
DROP TABLE IF EXISTS connector_webhook_events CASCADE;
DROP TABLE IF EXISTS connector_sync_logs CASCADE;

-- Drop functions
DROP FUNCTION IF EXISTS increment_batch_processed(UUID);
DROP FUNCTION IF EXISTS increment_batch_failed(UUID);

-- Revert connector_configs columns (optional - be careful!)
-- ALTER TABLE connector_configs DROP COLUMN IF EXISTS credentials_updated_at;
-- ... (only if rolling back completely)

-- Revert RLS policies
DROP POLICY IF EXISTS "Admins can manage their org's connectors" ON connector_configs;
DROP POLICY IF EXISTS "Service role full access to connectors" ON connector_configs;
DROP POLICY IF EXISTS "Service role full access to imported docs" ON imported_documents;

-- Delete storage bucket
DELETE FROM storage.buckets WHERE id = 'connector-imports';

RAISE NOTICE 'Phase 5 rollback completed';

COMMIT;
```

---

## üîí Part 7: Security Audit Results

### 7.1 Critical Security Issues

#### üî¥ CRITICAL: No Service Role Bypass for Background Jobs
**Impact:** Background workers will fail to insert/update records
**Affected Tables:** `connector_configs`, `imported_documents`, all new tables
**Risk Level:** HIGH
**Fix Priority:** P0

**Recommendation:**
```sql
-- Add service_role bypass to ALL connector tables
CREATE POLICY "Service role full access"
  ON {table_name} FOR ALL TO service_role
  USING (true) WITH CHECK (true);
```

#### üî¥ CRITICAL: Webhook Events Have No Deduplication Index
**Impact:** Duplicate webhook processing, data inconsistency
**Risk Level:** MEDIUM
**Fix Priority:** P0

**Current:**
```sql
UNIQUE (event_source, event_id)  -- Constraint exists but no index
```

**Recommended:**
```sql
CREATE UNIQUE INDEX idx_webhook_events_dedup
  ON connector_webhook_events(event_source, event_id);
```

#### üü° WARNING: Missing Role-Based Access Control
**Impact:** Any org member can create/delete connectors
**Risk Level:** MEDIUM
**Fix Priority:** P1

**Current:** All org members have full access
**Recommended:** Only owners/admins can manage connectors

### 7.2 Data Integrity Issues

#### Issue 1: No Cascade Delete for Connector Files
When a connector is deleted, imported documents remain but lose context.

**Recommendation:**
```sql
-- Option 1: Cascade delete (destructive)
ALTER TABLE imported_documents
  DROP CONSTRAINT imported_documents_connector_id_fkey,
  ADD CONSTRAINT imported_documents_connector_id_fkey
    FOREIGN KEY (connector_id)
    REFERENCES connector_configs(id)
    ON DELETE CASCADE;

-- Option 2: Soft delete (safer)
CREATE OR REPLACE FUNCTION soft_delete_connector_documents()
RETURNS TRIGGER AS $$
BEGIN
  UPDATE imported_documents
  SET is_deleted = true
  WHERE connector_id = OLD.id;

  RETURN OLD;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER before_delete_connector
  BEFORE DELETE ON connector_configs
  FOR EACH ROW
  EXECUTE FUNCTION soft_delete_connector_documents();
```

#### Issue 2: No Content Hash Collision Handling
Multiple files with same hash should be deduplicated.

**Recommendation:**
```sql
-- Add unique constraint with proper handling
CREATE UNIQUE INDEX idx_imported_docs_content_hash_org
  ON imported_documents(org_id, content_hash)
  WHERE content_hash IS NOT NULL;

-- Handle duplicates in application code
-- ON CONFLICT (org_id, content_hash) DO UPDATE SET last_synced_at = now()
```

### 7.3 Performance Security Issues

#### Issue 1: Missing Index on `processing_status`
Queries for pending documents will be slow.

**Impact:** Slows down background job processing
**Fix:**
```sql
CREATE INDEX idx_imported_docs_processing_status
  ON imported_documents(processing_status, org_id)
  WHERE processing_status IN ('pending', 'processing');
```

#### Issue 2: No Rate Limiting on Webhook Events
Webhook endpoint could be overwhelmed by malicious actors.

**Recommendation:**
- Implement rate limiting at application level
- Track `retry_count` and block after threshold
- Add `rate_limit_hit` tracking

---

## üìä Part 8: Performance Optimization Recommendations

### 8.1 Index Optimization

#### Recommended Additional Indexes

```sql
-- Connector sync scheduling
CREATE INDEX idx_connector_configs_active_sync
  ON connector_configs(is_active, next_sync_at)
  WHERE is_active = true AND sync_frequency != 'manual';

-- Document processing queue
CREATE INDEX idx_imported_docs_processing_queue
  ON imported_documents(org_id, processing_status, created_at)
  WHERE processing_status = 'pending'
  ORDER BY created_at ASC;

-- Webhook event processing queue
CREATE INDEX idx_webhook_events_processing_queue
  ON connector_webhook_events(processed, received_at)
  WHERE NOT processed
  ORDER BY received_at ASC;

-- Sync log analysis
CREATE INDEX idx_sync_logs_status_date
  ON connector_sync_logs(status, started_at DESC)
  WHERE status IN ('failed', 'partial');
```

### 8.2 Query Optimization

#### Issue: Nested Subqueries in RLS Policies
**Current:**
```sql
org_id IN (SELECT org_id FROM users WHERE id = auth.uid())
```

**Optimized:**
```sql
-- Create helper function
CREATE OR REPLACE FUNCTION auth.user_org_id()
RETURNS UUID AS $$
  SELECT org_id FROM users WHERE id = auth.uid() LIMIT 1;
$$ LANGUAGE sql STABLE SECURITY DEFINER;

-- Use in policies
CREATE POLICY "Users view their org's data"
  ON table_name FOR SELECT
  USING (org_id = auth.user_org_id());
```

### 8.3 Vacuum and Maintenance

```sql
-- Auto-vacuum settings for high-churn tables
ALTER TABLE connector_sync_logs SET (
  autovacuum_vacuum_scale_factor = 0.05,
  autovacuum_analyze_scale_factor = 0.02
);

ALTER TABLE connector_webhook_events SET (
  autovacuum_vacuum_scale_factor = 0.05,
  autovacuum_analyze_scale_factor = 0.02
);
```

---

## üéØ Part 9: Action Items & Priority Matrix

### P0 - CRITICAL (Must Fix Before Deployment)

| # | Issue | Impact | Effort | Files to Modify |
|---|-------|--------|--------|-----------------|
| 1 | Create missing tables | CRITICAL | 2h | Create migration 025 |
| 2 | Add missing columns to existing tables | HIGH | 1h | Migration 025 |
| 3 | Create helper functions | HIGH | 1h | Migration 025 |
| 4 | Add service_role RLS policies | HIGH | 30m | Migration 025 |
| 5 | Create connector storage bucket | HIGH | 30m | Migration 025_storage |
| 6 | Fix schema mismatches in services | MEDIUM | 1h | batch-uploader.ts, connector-manager.ts |

**Total Effort:** ~6 hours
**Blocker:** Services will FAIL without these changes

### P1 - HIGH (Should Fix Soon)

| # | Issue | Impact | Effort | Files to Modify |
|---|-------|--------|--------|-----------------|
| 7 | Add role-based access control | MEDIUM | 1h | Migration 025 |
| 8 | Implement content deduplication | MEDIUM | 2h | Migration 025, services |
| 9 | Add missing indexes | MEDIUM | 30m | Migration 025 |
| 10 | Create cleanup functions | LOW | 1h | New migration |

### P2 - MEDIUM (Nice to Have)

| # | Issue | Impact | Effort | Files to Modify |
|---|-------|--------|--------|-----------------|
| 11 | Implement unified search | LOW | 3h | New migration, search service |
| 12 | Add storage cleanup strategy | LOW | 2h | New migration, cron job |
| 13 | Optimize RLS policies | LOW | 1h | Migration 025 |

---

## üìù Part 10: Testing & Validation Checklist

### Pre-Migration Testing

- [ ] Backup database before running migration 025
- [ ] Test migration 025 in staging environment
- [ ] Verify rollback migration works correctly
- [ ] Check all indexes are created successfully

### Post-Migration Testing

#### Table Validation
- [ ] All 3 new tables exist (`connector_sync_logs`, `connector_webhook_events`, `file_upload_batches`)
- [ ] All columns added to `connector_configs`
- [ ] All columns added to `imported_documents`
- [ ] All constraints are active
- [ ] All indexes are created

#### Function Validation
- [ ] `increment_batch_processed` executes without errors
- [ ] `increment_batch_failed` executes without errors
- [ ] Both functions properly update progress_percent

#### RLS Policy Validation
- [ ] Service role can INSERT into all tables
- [ ] Service role can UPDATE all tables
- [ ] Regular users can SELECT their org's data
- [ ] Regular users CANNOT access other org's data
- [ ] Admins can manage connectors
- [ ] Non-admins cannot create connectors

#### Service Integration Testing
- [ ] ConnectorManager.createConnector() works
- [ ] ConnectorManager.syncConnector() logs to connector_sync_logs
- [ ] ConnectorManager.updateConnector() updates credentials_updated_at
- [ ] BatchUploader.processRequest() creates batch record
- [ ] BatchUploader increments processed/failed counts
- [ ] Storage uploads to connector-imports bucket

#### Performance Testing
- [ ] Check query performance on large datasets
- [ ] Verify indexes are being used (EXPLAIN ANALYZE)
- [ ] Monitor RLS policy overhead

---

## üö® Critical Blockers Summary

### Immediate Blockers (Must Fix Now)

1. **Missing Tables** - 3 tables referenced by services don't exist
2. **Missing Functions** - 2 RPC functions will cause runtime failures
3. **Missing Columns** - 15+ column references will fail
4. **Missing RLS Policies** - Background jobs will be blocked

### Migration Deployment Plan

```bash
# 1. Apply critical migration
psql $DATABASE_URL -f supabase/migrations/025_phase5_connector_system_enhancements.sql

# 2. Apply storage configuration
psql $DATABASE_URL -f supabase/migrations/025_phase5_storage_configuration.sql

# 3. Verify all changes
psql $DATABASE_URL -c "SELECT tablename FROM pg_tables WHERE tablename LIKE 'connector%' OR tablename LIKE 'file_upload%';"

# 4. Test RLS policies
# (Run test suite)

# 5. Monitor logs for errors
tail -f /var/log/supabase/postgres.log
```

---

## üìä Audit Score Card

### Overall Implementation Status

| Component | Completeness | Quality | Security | Performance | Final Grade |
|-----------|-------------|---------|----------|-------------|-------------|
| Database Schema | 60% | B | C | B | **C+** |
| RLS Policies | 40% | C | C | B | **C-** |
| Storage Integration | 20% | D | D | C | **D+** |
| Helper Functions | 0% | F | N/A | N/A | **F** |
| Service Integration | 70% | B | C | B | **B-** |

**Overall Phase 5 Grade: D+** (Incomplete, Critical Gaps)

### Risk Assessment

- **Deployment Risk:** üî¥ **CRITICAL** - Will fail in production
- **Data Integrity Risk:** üü° **MEDIUM** - No cascade deletes, weak deduplication
- **Security Risk:** üü° **MEDIUM** - Missing service_role policies, weak access control
- **Performance Risk:** üü¢ **LOW** - Adequate indexes once applied

---

## üéØ Conclusion & Next Steps

### Summary
The Phase 5 connector system has a **solid architectural foundation** but is **critically incomplete** in its database implementation. The services are well-designed and follow best practices, but they reference database structures that don't exist, causing guaranteed runtime failures.

### Immediate Actions Required

1. **Deploy Migration 025** (6 hours effort)
   - Creates 3 missing tables
   - Adds 15+ missing columns
   - Implements 2 helper functions
   - Fixes all RLS policies

2. **Update Service Code** (1 hour effort)
   - Fix `sync_status` ‚Üí `processing_status` mismatch
   - Ensure all column references are correct
   - Add proper error handling

3. **Storage Configuration** (30 min effort)
   - Create connector-imports bucket
   - Apply storage RLS policies
   - Update file path conventions

### Long-Term Recommendations

1. Implement comprehensive test suite for connector system
2. Add monitoring/alerting for sync failures
3. Create data retention policies for old imports
4. Build admin dashboard for connector management
5. Implement rate limiting for webhook endpoints

### Success Metrics Post-Fix

- ‚úÖ All 7 connectors operational
- ‚úÖ 100% uptime for background sync jobs
- ‚úÖ Zero RLS policy violations
- ‚úÖ < 1 second query performance for connector operations
- ‚úÖ 99.9% sync reliability

---

**Audit Completed By:** Claude (Supabase Specialist)
**Report Generated:** 2025-10-13
**Next Review:** After migration 025 deployment

**Critical Path:** Deploy migration 025 ‚Üí Test all services ‚Üí Monitor for 48 hours ‚Üí Move to production
